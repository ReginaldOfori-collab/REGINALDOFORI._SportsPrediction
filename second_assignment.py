# -*- coding: utf-8 -*-
"""SECOND ASSIGNMENT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h14hHp-FrKRPcPVU243s9j3Br4CGwr6U
"""

import pandas as pd
import numpy as np
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.impute import SimpleImputer
import category_encoders as ce
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import VotingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score, classification_report, confusion_matrix
from sklearn.model_selection import KFold, GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor

"""DATA PREPROCESSION"""

legacymaleplayers = pd.read_csv('legacymaleplayers.csv', low_memory = False, na_values = '-')
players = pd.read_csv('players_22-1.csv', low_memory = False, na_values = '-')

legacymaleplayers

players

legacymaleplayers.shape

legacymaleplayers.describe()

legacymaleplayers.shape

legacymaleplayers.drop(['player_id','player_url','fifa_version','fifa_update_date','player_face_url','fifa_update','dob','short_name', 'long_name', 'league_id','league_name','club_team_id','club_name','club_jersey_number','club_loaned_from','club_joined_date','club_contract_valid_until_year','nationality_id','nationality_name','nation_team_id','nation_jersey_number','real_face','body_type','release_clause_eur','player_tags','player_traits','mentality_composure', 'work_rate'], axis = 1, inplace = True)

legacymaleplayers

players.shape

players.columns







"""FEATURE ENGINEERING"""

l = []
l_less = []
for i in legacymaleplayers.columns:
    if ((legacymaleplayers[i].isnull().sum()<(0.3 *(legacymaleplayers.shape[0])))):
        l.append(i)
    else:
        l_less.append(i)

l

l_less

legacymaleplayers = legacymaleplayers[l]

legacymaleplayers

"""Separate Numeric and Non-Numeric Features"""

numeric_data = legacymaleplayers.select_dtypes(include = np.number)
non_numeric_data = legacymaleplayers.select_dtypes(include = ['object'])

imp = IterativeImputer(max_iter = 10, random_state = 0)
numeric_data = pd.DataFrame(np.round(imp.fit_transform(numeric_data)), columns = numeric_data.columns)

numeric_data

sc = SimpleImputer(strategy = 'median')
scaled = sc.fit_transform(numeric_data)

scaled

encoder = ce.BinaryEncoder(cols = non_numeric_data.columns)
non_numeric_data = encoder.fit_transform(non_numeric_data)
non_numeric_data.dropna(inplace = True)
non_numeric_data

legacymaleplayers = pd.concat([numeric_data, non_numeric_data], axis = 1).reset_index(drop = True)

corr_matrix = legacymaleplayers.corr()['overall'].sort_values(ascending = False)

corr_matrix

legacymaleplayers_usable = corr_matrix[:21]

legacymaleplayers_usable.index

legacymaleplayers = legacymaleplayers[legacymaleplayers_usable.index]

legacymaleplayers

numeric_data = players.select_dtypes(include = np.number)
non_numeric_data = players.select_dtypes(include = ['object'])

imp = IterativeImputer(max_iter = 10, random_state = 0)
numeric_data = pd.DataFrame(np.round(imp.fit_transform(numeric_data)), columns = numeric_data.columns)

sc = SimpleImputer(strategy = 'median')
scaled = sc.fit_transform(numeric_data)

encoder = ce.BinaryEncoder(cols = non_numeric_data.columns)
non_numeric_data = encoder.fit_transform(non_numeric_data)
non_numeric_data.dropna(inplace = True)
non_numeric_data

players = pd.concat([numeric_data, non_numeric_data], axis = 1).reset_index(drop = True)

players = players[legacymaleplayers_usable.index]

#Separating the overall column to scale the rest of the data
y = legacymaleplayers['overall']
x = legacymaleplayers.drop('overall', axis = 1)

scale = StandardScaler()
scaled = scale.fit_transform(x)

x = pd.DataFrame(scaled, columns = x.columns)

"""Training and Testing the Models"""

xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state = 42)

#LINEAR REGRESSION
l = LinearRegression()
l.fit(xtrain, ytrain)

#DECISION TREE REGRESOR
dtree = DecisionTreeRegressor()
dtree.fit(xtrain, ytrain)

#HARD VOTING
knn = KNeighborsRegressor()
sv = SVR()


# Creating a VotingRegressor with the specified models
voting_regressor = VotingRegressor(estimators=[
    ('dtree', dtree),
    ('knn', knn),
    ('sv', sv),
    ('l', l)
])

import pickle

voting_regressor.fit(xtrain, ytrain)
# #pkl.dump(voting, open('/content/' + voting._class_._name_ + '.pkl', 'wb'))
y_pred = voting_regressor.predict(xtest)
# print(voting_regressor(ytest, y_pred))

joblib.dump(voting_regressor, 'best_model.pkl')

print(f"""Voting Regressor Tree Mean Absolute Error = {mean_absolute_error(y_pred, ytest)},
        Voting Regressor Decision Tree Mean Squared Error = {mean_squared_error(y_pred, ytest)},
        Voting Regressor Decision Tree Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred, ytest))},
        Voting Regressor Decision Tree R2 Score = {r2_score(y_pred, ytest)}
""")

y_pred = l.predict(xtest)
print(f"""Linear Mean Absolute Error = {mean_absolute_error(y_pred, ytest)},
        Linear Mean Squared Error = {mean_squared_error(y_pred, ytest)},
        Linear Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred, ytest))},
        Linear R2 Score = {r2_score(y_pred, ytest)}
""")

y_pred = dtree.predict(xtest)
print(f"""Decision Tree Mean Absolute Error = {mean_absolute_error(y_pred, ytest)},
        Decision Tree Mean Squared Error = {mean_squared_error(y_pred, ytest)},
        Decision Tree Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred, ytest))},
        Decision Tree R2 Score = {r2_score(y_pred, ytest)}
""")

import xgboost as xgb
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import mean_squared_error, r2_score

# Initialize KFold with 3 splits
cv = KFold(n_splits=3)

# Initialize XGBoost Regressor
xgbr = xgb.XGBRegressor()

# # Define the parameter grid for GridSearchCV
# PARAMETERS_xgb = {
#     "max_depth": [1, 2, 3, 4],
#     "min_child_weight": [2, 4, 16],
#     "subsample": [0.8, 1],
#     "colsample_bytree": [0.8, 1],
#     "learning_rate": [0.3, 0.1, 0.03],
#     "n_estimators": [100, 500, 1000]
# }

# # Initialize GridSearchCV with XGBoost Regressor
# model_gs = GridSearchCV(xgbr, param_grid=PARAMETERS_xgb, cv=cv, scoring="neg_mean_squared_error", n_jobs=-1)
# model_gs.fit(xtrain, ytrain)

# Make predictions on the test set

xgbr.fit(xtrain,ytrain)
y_pred = xgbr.predict(xtest)

# Print the evaluation metrics
# print("Best parameters found: ", model_gs.best_params_)
# print("Best cross-validation score (neg_mean_squared_error): ", model_gs.best_score_)
print("Mean Squared Error on test set: ", mean_squared_error(ytest, y_pred))
print("R^2 Score on test set: ", r2_score(ytest, y_pred))

joblib.dump(xgbr, 'working_model1.pkl')

voting_regressor = VotingRegressor(estimators=[
    ('dtree', dtree),
    ('xgb', xgbr),
    ('l', l)
])

voting_regressor.fit(xtrain, ytrain)
# #pkl.dump(voting, open('/content/' + voting._class_._name_ + '.pkl', 'wb'))
y_pred = voting_regressor.predict(xtest)

print(f"""Voting Regressor Tree Mean Absolute Error = {mean_absolute_error(y_pred, ytest)},
        Voting Regressor Decision Tree Mean Squared Error = {mean_squared_error(y_pred, ytest)},
        Voting Regressor Decision Tree Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred, ytest))},
        Voting Regressor Decision Tree R2 Score = {r2_score(y_pred, ytest)}
""")

"""TESTING"""

testing = players

x1 = testing.drop('overall', axis=1)
y1 = testing['overall']

xtrain,xtest,ytrain,ytest=train_test_split(x1,y1,test_size=0.2,random_state = 42)

xtrain = scale.fit_transform(xtrain)
xtest = scale.fit_transform(xtest)

testing = voting_regressor.predict(xtest)

print(f"""Final Model Mean Absolute Error = {mean_absolute_error(testing, ytest)},
        Final Model Mean Squared Error = {mean_squared_error(testing, ytest)},
        Final Model Root Mean Squared Error = {np.sqrt(mean_squared_error(testing, ytest))},
        Final Model R2 Score = {r2_score(testing, ytest)}
""")

with open('best_model2.pkl', 'wb') as model_file:
    pickle.dump(voting_regressor, model_file)

with open('scaler.pkl', 'wb') as scaler_file:
    pickle.dump(scale, scaler_file)

